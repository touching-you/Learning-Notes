# GBDT

## GBDT基础

- 决策树
    
    [决策树 ](GBDT%2034700b50e5484946b53a60b748d7c153/%E5%86%B3%E7%AD%96%E6%A0%91%20ac0ba3e38e664b21b4fb431f40c693b8.md)
    
- 集成学习
    - Bagging
        
        $RF$(随机森林)
        
        弱模型（底层模型）并行，互不影响，最后共同决策。
        
        弱模型（底层模型）串行，前一个模型结果影响下一个模型的输入。
        
    - Boosting
        
        $AdaBoost、GBDT、XGBoost$ 
        
- 梯度下降
    - 梯度定义
        
        一阶泰勒展开如下：
        
        $$
        f(x) \approx f(x_0)+f'(x_0)(x-x_0)
        $$
        
        其中$f'(x_0)$为$f(x)$在$x_0$处的梯度
        
    - 梯度下降定义
        
        公式：
        
        $$
        x_{i+1}=x_i-\alpha f'(x_i)
        $$
        
        其中$\alpha$为步长，是一个常数。
        
        每次迭代更新$x_i$，对目标函数极小化，直到收敛。
        

## GBDT原理

## 提升树

- 提升树模型
    
    提升树模型可以表述为决策树加法模型：
    
    $$
    f_M(x) = \sum_{m=1}^M T_m(x)
    $$
    
    其中，$T_m(x)$表示第m个决策树，$M$为树的个数。
    
- 提升树算法
    - 回归算法
        
        输入训练集，$x_i\in X\subseteq\mathbb{R^n},y_i\in Y\subseteq\mathbb{R}$ 
        
        1. 初始化$f_0(x)=0;$ 
        2. 对$m=1,2,\dots,M$。
            1. 计算残差：
                
                $$
                r_{mi}=y_i-f_{m-1}(x_i), i=1,2,\dots,N
                $$
                
            2. 拟合残差$r_{mi}$学习一个回归树，得到$T_m(x)$ 。
            3. 更新$f_m(x)=f_{m-1}(x)+T_m(x)$ 。
        3. 得到问题回归树
            
            $$
            f_M(x)=\sum_{m=1}^M T_m(x)
            $$