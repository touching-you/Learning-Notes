# 决策树

### 决策树基础

- 最优码
  
    Kraft 不等式定理：对于D元字母表中的前缀码，码字长度为$l_1,l_2,\dots,l_m$必定满足不等式
    
    $$
    \sum D^{-l_i} \leq 1
    $$
    
    对于
    
    $$
    \begin{split}
    \hat C^* &= \arg \min L(X) \\
    &=\sum p(x)l(x)
    \end{split}\\
    s.t. \sum D^{-l(x)}\le 1
    $$
    
    利用拉格朗日乘子法可以得到，当$\hat l^*(x)=-\log_{D}p(x)$时取码字平均长度的最小值
    
    $$
    \begin{split}
    \hat L^*(X) &= -\sum p(x)\log_Dp(x) 
    \\&=H_D(X)
    \end{split}
    $$
    
    在信息论中最优码长取$-\log_2p_i$
    
- 信息熵
  
    对样本集$S$中的随机成员进行最优的编码时所需要的比特位数。则$S$的熵为：
    
    $$
    E(S) = -\sum_{i=0}^n p_i\log_2p_i=H(S)
    $$
    
    $S$的熵表示对每一种消息编码平均需要的比特数
    
    - 信息增益
      
        信息增益代表一个条件下，信息复杂度（不确定性）减小的程度。
        
        $$
        Gain(D,A) = H(D)-H(D|A)
        $$
        
        $H(D)$:熵（$D$的不确定性）
        
        $H(D|A)$:条件熵（在$A$条件下$D$的不确定性）
        
    - 信息增益比
      
        $$
        Gain_{Ratio}(D,A)=\frac{Gain(D,A)}{H_A(D)}
        $$
        
        其中
        
        $$
        H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|}
        $$
        
        $n$是特征$A$取值的个数
        
        - 基尼系数
          
            基尼系数表示随机取样的两样本不一致的概率。（越小越好）
            
            在分类问题中，假设有$M$个类别，第$k$个类别的概率为$p_k$，
            
            基尼系数公式为
            
            $$
            Gini(p)=\sum_{k=1}^M p_k(1-p_k) = 1-\sum_{k=1}^M p_k^2
            $$
            

### 决策树原理

![Untitled](Picture\Tree.png)

### 决策树分类

- 任务分类
    - 分类树
      
        特征分类依据：$Gini、Information Entropy、Information Entropy Rate$
        
    - 回归树
      
        特征回归依据：$MSE(Mean Square Error)$
    
- 特征方法分类
    - ID3决策树
      
        信息熵选择特征
        
    - C4.5决策树
      
        信息增益比选择特征
        
    - CART决策树
      
        $Gini$系数选择特征